{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syjoe02/Krx-Bench/blob/main/KRX_Bench_finance_language_model_instruction_tuning_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnZuU7PVtZF"
      },
      "source": [
        "# KRX-Bench 언어 모델 Instruction Tuning 튜토리얼\n",
        "- Unsloth와 금융 도메인 합성 데이터셋을 활용하여 효율적으로 모델을 학습시키는 튜토리얼입니다.\n",
        "- Unsloth는 적은 컴퓨팅 자원으로도 모델을 효율적으로 빠르게 학습시킬 수 있는 방법을 제공하는 라이브러리입니다.\n",
        "\n",
        "## 튜토리얼 환경 및 세부사항\n",
        "- 모델: [Meta-Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)\n",
        "- 데이터셋: [amphora/krx-sample-instructions](https://huggingface.co/datasets/amphora/krx-sample-instructions)\n",
        "- 학습 툴: [Unsloth](https://github.com/unslothai/unsloth)\n",
        "- 학습 방법 : Supervised Fine-tuning with QLoRA(4bit)\n",
        "- 학습 컴퓨팅 환경: Google Colab T4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjizHH4vVtZG"
      },
      "source": [
        "## 1. Unsloth 설치 및 학습 환경 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4DqZvT6VtZG"
      },
      "outputs": [],
      "source": [
        "# Unsloth 설치 및 최신 버전 업그레이드\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install trl bitsandbytes triton xformers peft\n",
        "\n",
        "# GPU가 softcapping을 지원하는 경우, Flash Attention 2 설치\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNfOabrCVtZH"
      },
      "source": [
        "## 2. 모델 및 데이터셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK96swGiVtZH"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None # None으로 지정할 경우 해당 컴퓨팅 유닛에 알맞은 dtype으로 저장됩니다. Tesla T4와 V100의 경우에는 Float16, Ampere+ 이상의 경우에는 Bfloat16으로 설정됩니다.\n",
        "load_in_4bit = True # 메모리 사용량을 줄이기 위해서는 4bit 양자화를 사용하실 것을 권장합니다.\n",
        "\n",
        "# 모델 및 토크나이저 선언\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # gated model을 사용할 경우 허깅페이스 토큰을 입력해주시길 바라겠습니다.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mlY9pMVtZH"
      },
      "outputs": [],
      "source": [
        "# LoRA Adapter 선언\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # 0을 넘는 숫자를 선택하세요. 8, 16, 32, 64, 128이 추천됩니다.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # target module도 적절하게 조정할 수 있습니다.\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # 어떤 값이든 사용될 수 있지만, 0으로 최적화되어 있습니다.\n",
        "    bias = \"none\",    # 어떤 값이든 사용될 수 있지만, \"none\"으로 최적화되어 있습니다.\n",
        "    use_gradient_checkpointing = \"unsloth\", # 매우 긴 context에 대해 True 또는 \"unsloth\"를 사용하십시오.\n",
        "    random_state = 42,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loDn97x9VtZH"
      },
      "source": [
        "## 3. 데이터셋 전처리\n",
        "데이터셋은 한국어 금융 합성 데이터셋인 `amphora/krx-sample-instruction`를 사용하였고, 데이터셋 템플릿으로는 Alpaca prompt에서 input을 제거한 형태의 프롬프트 템플릿을 사용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEDXXvLTVtZH"
      },
      "outputs": [],
      "source": [
        "prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"prompt\"]\n",
        "    outputs = examples[\"response\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = prompt_format.format(instruction, output) + EOS_TOKEN # 마지막에 eos token을 추가해줌으로써 모델이 출력을 끝마칠 수 있게 만들어 줍니다.\n",
        "        texts.append(text)\n",
        "    return { \"formatted_text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"amphora/krx-sample-instructions\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNT3z9nNVtZH"
      },
      "source": [
        "## 4. 모델 학습\n",
        "\n",
        "full train을 위해서는 `num_train_epochs=1`로 설정하고, `max_steps`를 설정하지 않으면 됩니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7K5p11fVtZH"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"formatted_text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # True로 설정하면 짧은 텍스트 데이터에 대해서는 더 빠른 학습 속도로를 보여줍니다.\n",
        "    args = TrainingArguments( # TrainingArguments는 자신의 학습 환경과 기호에 따라 적절하게 설정하면 됩니다.\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "97c2KpwfbNmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZl4bf7ZVtZI"
      },
      "source": [
        "## 5. 모델 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHa2tCFgVtZI"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt_format.format(\n",
        "        \"수요와 공급의 원리에 대해서 설명해줘.\", # instruction\n",
        "        \"\", # output 생성을 위해 빈 칸으로 설정\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ9S7MWeVtZI"
      },
      "source": [
        "## 6. 모델 저장 및 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsyXPKqVtZI"
      },
      "outputs": [],
      "source": [
        "# LoRA Adapter 저장\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "\n",
        "# Merged model 저장 및 업로드\n",
        "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\") # 개인 huggingface token을 사용하여 업로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 참고자료\n",
        "\n",
        "- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth Docs](https://docs.unsloth.ai/)\n",
        "- [Unsloth Meta-Llama-3.1-8B Finetuning Tutorial](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)\n",
        "- [Huggingface PEFT](https://github.com/huggingface/peft)\n",
        "- [Huggingface TRL](https://github.com/huggingface/trl)\n",
        "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
        "- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)"
      ],
      "metadata": {
        "id": "fAJJR1YVLYdg"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}